---
title: 浅谈高可用架构设计
date: 2023-01-18 17:30:32
permalink: /pages/4e98ab/
---

浅谈高可用架构 - 坚守 Design For Failure 的架构理念

初级研发工程师、高级研发工程师或者架构师等，负责的系统类型有：

- 负责一个功能
- 负责一个系统模块
- 负责一个系统
- 负责多个系统或业务条线

「高可用」这个概念，看起来很抽象，怎么理解它呢？通常用 2 个指标来衡量：
- **平均故障间隔 MTBF(Mean Time Between Failture)**：表示两次故障的间隔时间，也就是系统「正常运行」的平均时间，这个时间越长，说明系统稳定性越高。
- **故障恢复时间 MTTR(Mean Time To Repair)**：表示系统发生故障后「恢复的时间」，这个值越小，故障对用户的影响越小。

可用性与这两者的关系：
> 可用性（Availability） = MTBF / (MTBF + MTTR) * 100%

提升可用性的一个关键思路是：**冗余**。

我们都知道，系统发生故障其实是不可避免的，尤其是越大的系统，发生问题的概率也越大。这些故障一般体现在 3 个方面：
1. **硬件故障**：CPU、内存、磁盘、网卡、交换机、路由器
2. **软件问题**：代码 Bug、版本迭代
3. **不可抗力**：地震、水灾、火灾、战争

一般来说，建设一个机房的要求是非常高的，地理位置、温湿度控制、备用电源等等，机房厂商会在各方面做好防护。即时这样，我们每隔一段时间
还会看到这样的新闻：
- 2015年5月27日，杭州某地光纤被挖断，近 3 亿用户长达 5 小时无法访问支付宝
- 2021年7月13日，B 站部分服务器机房发生故障，造成整站持续 3 小时无法访问
- 2021年10月9日，富途证券服务器机房发生电力闪断故障，造成用户 2 个小时无法登录、交易
- ……

单机架构 -> 主从副本 -> 同城灾备 -> 同城双活 -> 两地三中心 -> 伪异地双活 -> 异地双活 -> 异地多活

参考文献：[两地三中心数据中心和同城双活数据中心的区别？](https://www.zhihu.com/question/48728378/answer/2176548792)


问题：怎么证明自己负责的系统是高可用的？

首先要保证系统的稳定性和可用性，然后才是基于高流量的场景下保证系统的并发承载能力。

SLA：服务等级协议规范了双方的商务关系或部分商务关系，你可以认为 SLA 是服务可用性一个重要衡量指标。

业界一般用几个 9 的 SLA 服务等级来衡量互联网应用的可用性，不可用时间是怎么计算出来的呢？

SLA = (1 - 年度不可用时间/年度总时间) × 100%

从公式中可以看出，SLA 等于 4 个 9，也就是可用时长达到 99.99%，不可用时长则为是 0.01%，一年是 365 天，8760 个小时，一年的不可用
时长就是 52.6 分钟。

SLA 等于 3 个 9，就相当于一年不可用时长等于 526 分钟。

SLA 等于 5 个 9，就相当于一年不可用时长等于 5.26 分钟。

可以发现，用 SLA 等于 4 个 9 作为参照物，少个 9 相当于小数点往后移一位，多个 9 相当于小数点往前移一位。

| 系统可用性 % | 宕机时间/年 | 宕机时间/月 | 宕机时间/周 | 宕机时间/天 |
|---|---|---|---|---|
|90%(1个9)|36.5天|72小时|16.8小时|2.4小时|
|99%（2个9）|3.65天|7.2小时|1.68小时|14.4分钟|
|99.9%（3个9）|8.76小时|43.8分钟|10.1分钟|1.44分钟|
|99.99%（4个9）|52.6分钟|4.38分钟|1.01分钟|8.66秒|
|99.999%（5个9）|5.26分钟|25.9秒|6.05秒|0.87秒|

**问题**：怎么设置这个指标的阈值才合理呢？

- 2个9表示系统基本可用，年度不可用时间小于88小时。
- 3个9较高可用，年度不可用时间小于9个小时。
- 4个9具有自动恢复能力的高可用，年度不可用时间小于53分钟。=》京东淘宝大部分是4个9。
- 5个9极高的可用性，年度不可用时间小于5分钟。

**问题**：你们的系统高可用做得怎么样？

流量的低峰期和高峰期分别停机 1 分钟，对业务影响的结果完全不同。

高可用评估：基于一段时间（比如 1 年）的停机影响的请求量占比，进行评估，公式如下：

高可用评估 = 停机时间影响请求量/总的请求量

先摆明度量的两种方式，"N 个 9" 和 "影响请求量占比"，然后在结合世家业务场景表明第二种方式的科学性。

闭环：
- 可评估
- 可监控
- 可保证

即业务关心的问题转化为：
- 如何评估系统高可用？
- 如何监控系统高可用？
- 如何保证系统高可用？

我们以设计一个保证系统服务 SLA 等于 4 个 9 的监控报警体系为例：

**监控系统**：
- 基础设置监控报警
- 系统应用监控报警
- 存储服务监控报警

**问题**：如果线上出现告警问题，你会如何处理？
对于线上故障，要有应急响应机制，具体包括：

**故障处理原则**
- 应急响应的目标：
    - 线上故障发生时，以快速恢复服务为第一优先级，避免或减少故障带来的损失，避免或减少故障对客户的影响。
    - 线上故障发生后，及时总结经验教训，提高团队的应急水平。
    - 线上故障发生前，积极预防，尽可能避免或减少故障发生。
- 应急响应的原则：
    - 首要任务，应在第一时间恢复服务。
    - 影响重大（比如受影响用户范围大，受损资金多，关键功能受阻等），应立即升级处理。
    - 如果不能短时间解决问题，应及时升级处理并尽可能止损。
- 应急响应流程：
    - 事前预防、问题监控、事中应对、故障定位、故障解决、事后总结、故障回顾、改进措施

可用性指标：

## 保证系统高可用的有效手段

案例：

网关系统 -》商品系统 -》促销系统/积分系统

出现流量高峰时，虽然商品系统很容易扩容，但对于商品依赖的其他服务，就不会有实时性的响应。那么出校或积分系统就可能因为无法
承担大流量，请求处理缓慢，直到所有线程资源被占满，无法处理后续的请求。

此时，积分系统的响应时间变长，其他依赖服务的整体请求的响应时间也会因此变长，整体服务甚至会发生宕机。即服务雪崩。

**雪崩现象**：即局部故障最终导致了全局故障。

要怎么避免雪崩呢？对于系统可用性，你要通过三个方面来解决：分别是"评估"、"检测" 和 "保证"。

**解决的思路是**：在分布式系统中，当检测到某一个系统或服务响应时长出现异常时，要想办法停止调用该服务，让服务的调用快速返回失败，
从而释放此次请求持有的资源，这就是架构设计中经常提到的降级和熔断机制。

**熔断设计的原理**：参考了电路中保险丝的保护原理。在微服务架构中，服务的熔断机制是指：在服务 A 调用服务 B 时，如果 B 返回错误或超时
的次数超过一定阈值，服务 A 的后续请求将不再调用服务 B。这种设计方式就是断路器模式。

**降级设计的原理**：降级设计是站在系统整体可用性上考虑问题：当资源和访问量出现矛盾时，在有限的资源下，放弃部分非核心功能或者服务，保证
整体的可用性，熔断也是降级的一种手段。


## 怎么做降级设计？
降级设计：
- 服务降级
    - 读操作降级：做数据兜底服务，将兜底数据提前存储在缓存中，当系统触发降级时，都操作直接降级到缓存，从缓存中读取兜底数据，如果此时缓存中也不存在查询数据，则返回默认值，不再请求数据库。
    - 写操作降级：将之前直接同步调用写数据库的操作，降级为先写缓存，然后再异步写入数据库。
- 功能降级：
  就是在做产品功能上的取舍，既然在做服务降级时，已经舍掉了非核心的服务，那么同样的产品功能层面也要相应的进行简化。可以通过简化降级开关控制功能的可用或不可用。另外，在设计降级时，离不开降级开关的配置，一般是通过参数化配置的方式存储在配置中心，手动或自动开启开关，实现系统降级。

其他高可用设计方案：
- 服务冗余
- 负载均衡
- 故障隔离
- 限流

应对连锁故障：
- 避免过载
- 优雅降级
- 重试退避
- 超时控制
- 变更管理
- 极限压测 + 故障演练
- 扩容 + 重启 + 消除有害流量

架构高可用的手段
- 设计无状态化
- 子系统冗余
- 幂等性设计
- 异步调用
- 超时机制
- 分级管理
- 服务降级
- ……


系统整体架构层面：
- 硬件
- DNS
- CDN
- 接入层
- 逻辑层
- 数据存储层
- 分布式缓存层
- 数据层
- ……

## 高可用架构知识图谱

高可用
- 可用性度量
    - 可用性指标
    - 故障分类
- 高可用架构
    - 负载均衡
    - 备份与失效转移
    - 消息队列隔离
    - 限流与降级
    - 异地多活
- 高可用运维
    - 自动化部署
    - 自动化监控
    - 自动化测试
    - 预发布测试

### 系统可用性的挑战
- DNS 被劫持
- CDN 服务不可用
- 应用服务器及数据库服务器宕机
- 网络交换机失效
- 硬件故障：硬盘损坏、网卡松掉
- 环境故障：机房停电、空调失灵、光缆被挖掘机挖断
- 代码 bug
- 黑客攻击
- 促销引来大量用户访问
- 第三方合作伙伴的服务不可用

### 互联网应用可用性的度量
#### 故障分类
|分类|描述|权重|
|---|---|---|
|事故级故障|严重故障，网站整体不可用|100|
|A类故障|网站访问不顺畅或核心功能不可用|20|
|B类故障|非核心功能不可用，或核心功能少数用户不可用|5|
|C类故障|以上故障以外的其他故障|1|
故障分 = 故障时间 × 故障权重

#### 故障处理流程
客户报告故障或监控系统发现故障（故障开始时间）-> 提交故障给相关部门接口人 -> 故障接手&处理 -> 故障处理完毕，故障归档（故障结束时间）-> 确认故障归属记入绩效考核

### 系统高可用的一般策略
- 应用服务器负载均衡：即将用户的请求通过负载均衡服务器分发到多个web服务器上，负载均衡服务器还可以保证单某个web服务器不可用（也包括应用程序发布）时，能够自动将该部分流量转发到其他web服务器上。
    - HTTP 负载均衡
    - DNS 负载均衡：用户浏览器(用户请求域名解析) -> DNS 服务器 ->返回 IP 地址 -> 用户使用真实 IP 进行浏览器请求 -> Web 服务器集群
    - 反向代理负载均衡：
    - IP 层负载均衡：
    - 数据链路层负载均衡：（用的最多的方案）
    
大型互联网一般采用两级负载均衡，DNS 服务器解析出来的 IP 地址是负载均衡服务器的 IP 地址，这样就不会将真实的服务器的 IP 地址暴露出来。

#### 数据库的高可用
读写分离：保证读和写的资源隔离。
- **读高可用**：通过数据库的主从模式，多个从服务器，当读操作读取的某个从服务器挂掉，会迁移到其他可用的从服务器上。
- **写高可用**：通过数据库的主主模式，即正常操作的时候写操作会写到主服务器 A，当主服务器 A 失效的时候，写操作会被发送到主服务器 B。

#### 消息队列隔离
通过消息队列来实现异步解耦。生产者和消费者通过消息队列进行解耦，当消费者发生故障的时候，生产者可以继续向消息队列发送数据，而不会影响生产者。

另外，还可以进行削峰填谷的作用。


#### 限流和降级
- **限流**：通过对并发访问进行限流，降级并发请求的数量来保护系统。
- **降级**：关闭部分非核心功能，降低对系统的资源消耗，保证系统在高并发的情况下仍然保持可用。


#### 异地多活机房架构
避免数据修改冲突，类似 MySQL 的主主模式。

#### 高可用运维
- 自动化测试：
- 自动化监控
- 预发布：比如有一台预发布服务器，不和域名解析和负载均衡服务器连接在一起，但是却有线上的机器的所有环境，只有内部的工程师才可以访问到。
- 灰度发布：灰度就是在生产环境进行小范围测试 （这个观点是错误的），它本身是为了对抗"未知的不确定性"，需要更加谨慎地进行灰度，确保即使问题真的在生产环境出现，造成的影响也是可控的。


## 深入业务是做好高可用架构的前提
架构设计与系统演进：架构要结合具体的业务场景来设计。

**理解业务这件事**：
- 产品需求不等于业务诉求
    - 产品提出要实现的系统功能未必等于业务想要解决的问题
    - 怎么在规定的时间内搞定这个需求

产品需求：
- 1 W 载客量
- 200 km/h 速度
- 10 级 抵抗风浪

开发侧的理解：
- 需要多少钢材
- 多少工人、几个发动机
- 船舱结构要如何设计

实际上业务的诉求是："安全达到对岸"，业务其实不关心你是修船、造车、开车。

实际的开发过程中很多人不关心需求的源头，不能真正理解业务。

大部公司中，技术处于价值创造的末端。

用户的真实需求 -> 业务 -> 运营 -> 产品 -> 技术

每一层信息都会被加工、处理、拆分，技术看到的问题距离最想解决的问题可能会很远，没有搞清楚问题的源头而去解决问题，结果会很糟糕。

订单作为交易的载体需要承载大量的数据：
1. 订单系统的演进完全跟着业务需求走。
2. 很难判断是否应该让这些数据落到订单上。（存储在一个无法管理的 JSON 字段中）

表面上看，是系统设计和实现不够好，实际上是：没有在深度理解业务的基础上对交易系统进行建模，确定边界与能力范围。

**深度理解业务**
- 业务的现状
- 推测业务的发展
- 思考业务上对交易的诉求

通过理解业务可以让团队的同学进一步了解自己工作的价值和意义，提升技术团队的使命感。

技术同学对业务的直观感受大多来自线上的产品和系统，这和直接接受用户有很大的差别。

技术团队  ->沟通  一线的客服人员，技术团队会遭到来自用户和客服的双向暴击。

阿里 P7 以上的技术人员能否晋升，深度理解业务很重要。

**如何理解业务的小技巧**：
- 不要盲信产品   "永远不要试图用战术上的勤奋，去掩盖你战略上的懒惰"
- 技术团队要建立走进业务的机制
- 实际去体验业务会让你建立很强的认识感与同理心
- 只有站在他们的角度你才能看到他们的痛点，才会思考技术是不是能解决你原本未必知道或者关注的问题

**不要让业务机制成为"一次性作秀"**

**可回滚的设计**：

可回滚的本质是系统的兼容性设计与实现：
- 比如常见的"只增不改"


## 坚守 Design For Failure 的架构理念
"Design for failure and nothing will fail" 最早是 AWS 的一条最佳实践，即面向失败进行系统设计。

- 考虑系统所有可能发生故障或不可用的情形，并假设这些可能都会发生，倒逼自己设计足够健壮的系统。
    - 非关键路径都要可以降级
    - 核心系统一定要有熔断、限流、超时这些保护手段
    - 架构上要避免单点
如何推行并落地这种理念：
- **正向**：如何形成 Design For Failure 的系统设计习惯？
- **方向**：如何确定系统真的可以 Failover?

历史是最好的老师，总结分析过去发生的事故，并沉淀相关的经验，以此梳理出围绕事故隐患的风险点 CheckList（<mark>这个工作看着挺有价值</mark>）

- DB
    - 容量
        - 主从：1. MHA 风险；2. 同步延迟
        - 分布分表：热点
        - 预估增长：1. ID 溢出；2. 记录 > 1KW
    - 性能
        - 慢 SQL：1. 限定条件缺失（select *；日期条件） 2. 索引缺失
        - SQL 阻塞：多线程读表、锁、连接数（池）

- 可用性治理于预防
    - 变更三板斧
        - 坚决不允许"三无发布"，红线问责！
        - 可监控：系统、业务、覆盖
        - 可灰度：用户、接口、服务器
        - 可回滚：架构兼容
    - Design For Failure:
        - 稳定性 CheckList
        - 沉淀设计原则


比如：超出预期的主从延迟是分布式系统中很可能出现的情况。

- 如果业务场景上主从延迟的容忍度很高，还不是关键路径，做好降级开关可能就足够了。
- 如果是写完即读的场景，就要考虑是不是让读请求直接绑定主库，并且对主库是否造成较大压力以及缓存是否能够起到作用，或者改为消息推送的方式。

### 通过演练验证预案设计
Design For Failure
- 这些措施在故障发生时是否真的有效？
- 处理流程与沟通协作是否通畅？
即故障演练系统。

技术 Leader 要化被动为主动，有意识地推进故障演练，不论是以注入还是回放的形式制造可控的故障，以此验证应急处理的机制流程和预先设计的灾备方案是否有效。

演练的演化过程：

现在测试环境检验，后面才开始在生产环境进行有预案的演练，最后才有可能进行真正的随机故障演练。

希望大家不要用一次次的重大事故来让团队成员慢慢理解系统稳定性的重要性。


### 证明保障
- 冗余无单点：不仅是机器，还包括网络，比如：通信线路不仅要有移动的也要有联通的，再有异地多活等架构。
- 水平扩展：无状态的计算节点容易扩展，而数据库则通过水平分库来实现。


## 程序优化
代码优化：
- 表单压缩
- 局部刷新
- 仅取所需
- 逻辑清晰
- 谨慎继承
- 批处理
- 延迟加载
- 防止内存泄漏
- 减少大对象引用
- 防止争用死锁
- 索引
- 并行
- 异步
- 设计模式

配置优化：
- JVM 配置优化
- 连接池：
- 线程池：
- 缓存机制：

## 性能测试




















































